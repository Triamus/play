---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

- [jaceklaskowski: Mastering Apache Spark](https://legacy.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details)
- [Spark Docs: Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html)
- [How to set up PySpark for your Jupyter notebook](https://medium.freecodecamp.org/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389)
- [Setting up a scalable data exploration environment with Spark and Jupyter Lab](https://becominghuman.ai/setting-up-a-scalable-data-exploration-environment-with-spark-and-jupyter-lab-22dbe7046269)
- []()
- []()
- []()
- []()


# Install Spark

* [Medium: Installing Scala and Spark on Ubuntu](https://medium.com/@josemarcialportilla/installing-scala-and-spark-on-ubuntu-5665ee4b62b1)
* [Medium: Install Spark on Ubuntu (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0)

## download and extract

```{bash}
wget http://ftp.halifax.rwth-aachen.de/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz

tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz
```

## set environment/path variables

We work with symbolic (soft) links which make it easier to switch to different verisons of a particular program without having to adjust the path variables every time. In our case, we create a symbolic link directory called `spark-current` which points to the Spark version we want to use as default (in our case `spark-2.3.0-bin-hadoop2.7`. Whenever we like to change the default Spark version, we only have to change the symbolic link rather than all dependencies such as path and home variables.

We create a symbolic link as so

```{bash}
ln -s <real folder> <link folder>
# in our case
ln -s spark-2.3.0-bin-hadoop2.7 spark-current
# to switch existing symlink to another target, remove the directory symlink first recursively
sudo rm -r spark-current
ln -s spark-2.2.1-bin-hadoop2.7 spark-current
```

You can check if the operation was successful by listing the directory contents via `ls -l` and the directory `spark-current` should display a right pointing arrow to its link directory, e.g. here `spark-2.3.0-bin-hadoop2.7`. If you were to swith into the `spark-current` directory you will actually be directed to its link reference. The same is true for any other operation pointing to the `spark-current` directory such as file operations, etc.

Finally, we adjust our path within `.bashrc` by adding below (adjust for your actual paths).

```{bash}
export SPARK_HOME="/home/triamus/dstools/spark-current"
export HADOOP_HOME="/home/triamus/dstools/spark-current/bin"
export PATH="/home/triamus/dstools/spark-current/bin:$PATH"
```

To run PySpark from a Jupyter notebook (`"notebook"`) or from Jupyter Lab (`"lab"`), we also add the following (Note that this will depend on the python environment you choose to work with pyspark. If you are not leveraging virtual Python environments at all, simply use the default Python).

```{bash}
function sparklab () 
{
#Spark path
SPARK_PATH="/home/triamus/dstools/spark-current"

# when using a specific environment
export PYSPARK_DRIVER_PYTHON="$ANACONDA_ROOT/envs/dspy36/bin/jupyter"
export PYSPARK_PYTHON="$ANACONDA_ROOT/envs/dspy36/bin/python"
export PYSPARK_DRIVER_PYTHON_OPTS="lab"
#export PYSPARK_DRIVER_PYTHON_OPTS="notebook"

# when using python3 default
#export PYSPARK_PYTHON=python3
#export PYSPARK_DRIVER_PYTHON="jupyter"
#export PYSPARK_DRIVER_PYTHON_OPTS="lab"
#export PYSPARK_DRIVER_PYTHON_OPTS="notebook"

# starting pyspark with defined number of cores
$SPARK_PATH/bin/pyspark --master local[4]
}
```

Running PySpark should now automatically open in a Jupyter notebook.

```{bash}
pyspark
#exit()
```

# Install Anaconda

* [How To Install the Anaconda Python Distribution on Ubuntu 16.04](https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-16-04)

In case not present, install Anaconda from the [official distribution](https://www.anaconda.com/download/). Restart the shell after the installation is done to make new path variables available.

# Install Jupyter Lab

* [Github: JupyterLab](https://github.com/jupyterlab/jupyterlab)

We create a new conda [environment](https://docs.anaconda.com/ae-notebooks/user-guide/adv-tasks/work-with-environments/) to isolate any development (adjust libraries as you require). Jupyterlab comes with the Anaconda distribution. If you require a different version, you can run the last statement explicitly within the newly created environment.

```{bash}
conda env list
# this will be a big install so it may take some time
conda create --name dspy36 python=3.6 anaconda python-blosc azure
# on linux, macos activate the environment
source activate dspy36
# in case jupyterlab is not already part of the distribution, install it manually
conda install -c conda-forge jupyterlab
# to allow switching Python virtual environments, also install nb_conda in case it's not present
conda install nb_conda
# we also install findspark to find a Spark cluster from within our jupyter session
conda install -c conda-forge findspark
```

We can launch Jupyter by issuing below command in the created environment shell. It should automatically launch in your default browser.

```{bash}
jupyter lab
#jupyter notebook
```

However, to actually use our created environment with all associated variables, we make use of our created Bash function that we defined in `.bashrc`.

```{bash}
sparklab
```

JupyterLab should open in your browser with the correct environment running. If you installed `nb_conda` you should also be able to switch environments from within JupyterLab or create a new notebook from an environment of your choice.

To avoid confusion, you may change the name of shown environment in the notebook/lab from only `Python` to `Python (myenv)` as so (from command line).

```{bash}
source activate myenv 
python -m ipykernel install --user --name myenv --display-name "Python (myenv)"
```

The selection in Jupyter should now show the full name.

## try later - didnt work

[GitHub: jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic)

```{bash}
# install will have a few dependencies
conda install -c conda-forge sparkmagic

```


# Programming IDE for Spark

There are many ways to program against Spark. We will use [Apache Zeppelin](https://zeppelin.apache.org/).

Start it as follows. The notebook should then be exposed under `http://localhost:8080`.

```{bash}
bin/zeppelin-daemon.sh start
```

# Programming API for Spark

* [Scala vs. Python for Apache Spark](https://dzone.com/articles/scala-vs-python-for-apache-spark)

```{bash}
./spark-current/bin/spark-shell
# to exit shell :q, :quit, sys.exit
```

