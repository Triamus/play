---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

I wanted to look into Apache Drill for some time and after several endorsements from [Bob Rudis](https://rud.is/b/) and his [announcement](https://rud.is/b/2017/07/17/ten-hut-the-apache-drill-r-interface-package-sergeant-is-now-on-cran/) that his R interface package [sergeant](https://github.com/hrbrmstr/sergeant) is now on CRAN there was no excuse for further procrastination. And I have to admit that it did not take long to convince me that this is an indispensable tool for any data worker. In this post I will look into Apache Drill in general, a connection via Apache Zeppelin and the R interface package `sergeant`.

[Apache Drill](https://drill.apache.org/) is an open source distributed SQL query engine with roots in the [MapR](ecosystem). MapR summarizes the [benefits](https://mapr.com/products/apache-drill/) of Drill in 10 points (with some MapR bias but nontheless informative)

* High Performance SQL Queries with Scale-Out Architecture
    + Apache Drill can support thousands of users across thousands of nodes running queries on data that is in the terabyte and petabyte range.
* Schemaless Query Execution for Data Exploration
    + discover schemas on the fly and enable immediate exploration of data stored in MapR across a variety of data formats and sources.
* In-Place Analytics across Historical and Operational Data
    + No need to move data between operational and analytics clusters.
* Connectivity to Popular BI Tools through QDBC and JDBC interfaces
    + Connect with popular BI tools, such as Tableau, MicroStrategy, Qlik, and many more.
* ANSI SQL Compliance
    + All the SQL analytics functionality you would expect, such as aggregates, filters, sorting, sub-queries (scalar and correlated), create table/ view as, etc., is available out of the box.
* Integration with MapR-DB Secondary Indexes for Operational Analytics
    + Up to 10X query performance improvement due to native integration with MapR-DB, including secondary indexes.
* Integration with Hive for Interactive Queries on Existing Hive Tables
    + Continue querying existing Hive tables. No disruptions to existing BI workflows.
* Cluster Health and Resource Monitoring through MapR Control System
    + MCS gives you a single pane of glass for cluster metrics, alarms, and service logs as well as a curated user experience with streamlined workflows for common user actions.
* Integration into MapR Data Science Refinery for Augmenting Data Science Workflows
    + Direct integration into the MapR Data Science Refinery enables self-service data exploration for data scientists, leading to better models.
* End-To-End Security for Data Accessed, Processed, and Analyzed
    + Versatile authentication mechanisms–PAM, Kerberos, and MapR Security. State-of-the-art encryption to protect sensitive data with SSL and AES 256 GCM support.

# Installation

Apache Drill can be used in embedded mode (standalone machine) on Linux, Mac and Windows machines and in distributed mode on a cluster. You can build it from source or download compiled binaries. You don't necessarily need admin rights to run it but it has some system requirements that need to be in place, in particular the JDK and some environment variables. Detailed instructions can be found under [Installing Drill in Embedded Mode](https://drill.apache.org/docs/embedded-mode-prerequisites/). In general you will find an excellent documentation on the [Apache Foundation page](https://drill.apache.org/docs).

# A quick introduction to Apache Drill

Assuming you have installed Apache Drill and running it in embedded mode on a single machine, let's try out some of the useful utilities it provides. First, we start Drill via the command line (in our case Bash on Linux but CMD on a Windows machine will be similar). We direct the console to the installation path subdirectory `/bin` and issue below command.

```{bash}
drill-embedded
# exit the shell via !quit
```

Or on Windows.

```{bash}
sqlline.bat -u "jdbc:drill:zk=local"
```

![](D:/Sonstiges/screenshots/cmd_start_drill.png)

The web UI is hosted by default on `port 8047` so direct a browser of your choice to `http://localhost:8047/` to open it.

![](~/screenshots/chrome_drill_startpage.png)

If we want to interact with the file system, we may have to adjust a few configuration settings in the storage pane, in particular the following in `dfs`

```{r, eval=FALSE}
"tmp": {
  "location": "~/data",
  "writable": true,
  "defaultInputFormat": null,
  "allowAccessOutsideWorkspace": false
},

...

"csv": {
  "type": "text",
  "extensions": [
    "csv"
  ],
  "skipFirstLine": true,
  "extractHeader": true,
  "delimiter": ","
},

...
```

![](~/screenshots/chrome_drill_storage_config.png)

We create a csv file from R `datasets::mtcars` but you can also download it e.g. from this [GitHubGist](https://gist.github.com/seankross/a412dfbd88b3db70b74b).

https://www.rittmanmead.com/blog/tag/apache-drill/
https://blog.ouseful.info/2017/06/03/querying-large-csv-files-with-apache-drill/
http://www.treselle.com/blog/drill-data-with-apache-drill-part-2/
https://statcompute.wordpress.com/2017/12/17/query-csv-data-with-apache-drill/
https://rud.is/b/2016/12/20/sergeant-a-r-boot-camp-for-apache-drill/
https://blogs.technet.microsoft.com/machinelearning/2016/12/14/exploring-azure-data-with-apache-drill-now-part-of-the-microsoft-data-science-virtual-machine/
https://gallery.cortanaintelligence.com/Tutorial/Data-Exploration-on-the-Data-Science-Virtual-Machine-using-Apache-Drill-1

```{r}
data("mtcars")
readr::write_csv(mtcars, "~/data/other/mtcars.csv")
str(mtcars)
```

We create a `parquet` file via Drill to demonstrate a multi-source query. We can test the file system connection via a simple select of a parquet file that ships with the Drill installation. We access the file system via the `dfs` prefix.

```{sql}
select
  * 
from
  dfs.`home/triamus/apache-drill-1.13.0/sample-data/region.parquet`;
```

![](~/screenshots/drill_example_query_parquet.png)

We create the parquet file from the csv.

```{sql}
create table
  dfs.tmp.`/other/mtcars.parquet` 
as select
  * 
from 
  dfs.tmp.`/other/mtcars.csv`
```

Finally, we perform the cross-file query.

```{sql}
select 
  csv.cyl, 
  csv.gear,
  parquet.hp,
  parquet.drat
from 
  dfs.tmp.`/other/mtcars.csv` as csv
left join
  dfs.tmp.`/other/mtcars.parquet` as parquet
on
  csv.mpg = parquet.mpg
where
  csv.gear = 4
limit 5
```

# Connect Apache Zeppelin with Drill via JDBC

Rather than using the web query interface on an ad-hoc basis, we can connect to Drill with a Apache Zeppelin notebook using the JDBC driver. Assuming Zeppelin is installed and the JAVA_HOME variable is set, simply create a new interpreter in Zeppelin (in our example called `drill`) with reference to the 

* default.driver: `org.apache.drill.jdbc.Driver`
* default.url: `jdbc:drill:drillbit=localhost`
* default.user: e.g. admin
* zeppelin.interpreter.localRepo: e.g. /home/triamus/dstools/zeppelin-0.7.3-bin-all/local-repo/2DA2CF5MW
* artifact (location of Drill JDBC driver): e.g. /home/triamus/dstools/apache-drill-1.13.0/jars/jdbc-driver/drill-jdbc-all-1.13.0.jar

We show an example config below. For details on setting up a Zeppelin-Drill connection in embedded or distributed mode refer to the [documentation](https://drill.apache.org/docs/using-the-jdbc-driver/) and for troubleshooting, see e.g. [Apache Drill - connection to Drill in Embedded Mode](https://stackoverflow.com/questions/31654658/apache-drill-connection-to-drill-in-embedded-mode-java). You may also consult [Generic JDBC Interpreter for Apache Zeppelin](https://zeppelin.apache.org/docs/0.6.1/interpreter/jdbc.html).

![](~/screenshots/zeppelin_drill_interpreter_config.png)

We run a short example query in the Zeppelin notebook such as 
```{sql}
%drill

select
  * 
from 
  cp.`employee.json` 
limit 3
```

![](~/screenshots/zeppelin_drill_example_query.png)

# Sergeant (An R interface to Drill)

Taken from `sergeant` github page, it is described as 

> Drill + sergeant is a nice alternative to Spark + sparklyr if you don't need the ML components of Spark (i.e. just need to query "big data" sources, need to interface with parquet, need to combine disparate data source types — json, csv, parquet, rdbms - for aggregation, etc). Drill also has support for spatial queries.

Sergeant currently does not cover the full spectrum of Drill but the authors invite PRs for feature additions in future releases. 

Interface-wise, we can leverage (limited) dplyr functionality via R DBI driver, directly use DBI or connect via JDBC through e.g. RJDBC.

For more details visit the [Github page](https://github.com/hrbrmstr/sergeant) or consult the [CRAN vignettes](https://cran.r-project.org/web/packages/sergeant/).

```{r}
library(sergeant)
packageVersion("sergeant")
```

A `dplyr` conection through `DBI` is initiated via `sergeant::src_drill()` (with some limitations on query complexity).

```{r}
ds <- src_drill("localhost")
ds
```

We can assign a table.

```{r}
db <- tbl(ds, "dfs.tmp.`/other/mtcars.csv`")
db
```

Run a simple `dplyr` chain.

```{r}
#library(dplyr)
db %>%
  filter(gear > 3 & cyl < 8)
```

We create a Drill connection through the REST API.

```{r}
dc <- drill_connection("localhost") 
drill_version(dc)
```

The workhorse function is `sergeant::drill_query()` which sends respective query through the connection either created via `sergeant::drill_connection()` i.e. running through REST API or `sergeant::drill_jdbc()` i.e. running through JDBC connection. It currently returns a `dplyr::tbl_df()`.

We illustrate usage through a simple query with Drill's example files.

```{sql}
drill_query(
  dc, 
  "select
     count(gender) as gender 
   from 
     cp.`employee.json`
   group by
     gender")
```

To make a final point, we re-run the earlier Zeppelin query via sergeant.

```{sql}
drill_query(
  dc,
  "select 
     csv.cyl, 
     csv.gear,
     parquet.hp,
     parquet.drat
   from 
     dfs.tmp.`/other/mtcars.csv` as csv
   left join
     dfs.tmp.`/other/mtcars.parquet` as parquet
   on
     csv.mpg = parquet.mpg
   where
     csv.gear = 4
   limit 5;"
  )
```

# Drill vs Spark

As with many tools in the big data ecosystem Drill has overlaps with other (SQL-on-Hadoop) tools such as [Impala](https://impala.apache.org/), [Presto](https://prestodb.io/), [Hive](https://hive.apache.org/), [Quasar](http://quasar-analytics.org/) or [SparkSQL]()https://spark.apache.org/sql/. In particular Spark is often seen as the center of the new open source big data eco-system.

![](https://zdnet3.cbsistatic.com/hub/i/r/2017/11/01/sparkecosystem.png)

[Spark SQL](https://spark.apache.org/sql/)
http://spark.apache.org/docs/latest/sql-programming-guide.html


* [Big Sata Path: Apache Spark vs. Apache Drill](https://bigdatapath.wordpress.com/2018/03/10/apache-spark-vs-apache-drill/)
* [MapR: Apache Spark vs. Apache Drill](https://mapr.com/blog/apache-spark-vs-apache-drill/)
* [SlamData: Battle of Open Source Analytics: Spark vs Drill vs Quasar](https://slamdata.com/news-and-blog/2016/07/20/battle-of-open-source-analytics-spark-vs-drill-vs-quasar/)
* [Dezyre: Spark SQL vs. Apache Drill-War of the SQL-on-Hadoop Tools](https://www.dezyre.com/article/spark-sql-vs-apache-drill-war-of-the-sql-on-hadoop-tools/234)
So the question arises how Drill is different from Spark. In general Drill is a distributed high-performance full SQL engine while Spark is a distributed high-performance general computing engine with limited SQL support. In particular, one can find arguments such as (only applies to direct SQL capability/exceution comparison)

* Drill supports full ANSI SQL:2003 while Spark SQL only supports a fraction
* Drill supports special json operations such as `flatten`, Spark SQL also added this now
* Drill integrates with many BI tools
* Spark SQL inputs have to be loaded into the session, i.e. load time with many files will come into play
* Spark SQL is embedded in another language e.g. Scala
* Drill optimizer is more nuanced and thus performs better
* Drill has lower memory usage (also no Java garbage collector)
* Drill comes with more capable security model (e.g. via views)
* Apache Arrow started as sub-project out of Drill
* Drill can be integrated into Spark (e.g. via JDBC, DrillContext)
* both allow user-defined functions

The aim is not to provide any performance tests here but a naive guess is that once all data is in memory, Spark SQL shouldn't perform much worse than Drill (or even better?). What will likely pull down Spark performance are cases with many file reads as Drill executes directly on those while Spark loads them all into memory first. One comparison can be found in [Comparing SQL Functions and Performance with Apache Spark and Apache Drill](https://mapr.com/blog/comparing-sql-functions-and-performance-apache-spark-and-apache-drill/).

Going back to our earlier example, a Spark SQL query would look something like below. First, we need to load the data into our session before we can access it.

```{sql}
%spark.sql

-- mode "FAILFAST" will abort file parsing with a RuntimeException if any malformed lines are encountered
CREATE TEMPORARY TABLE temp_mtcars_csv
  USING csv
  OPTIONS (path "/home/triamus/data/other/mtcars.csv", header "true", mode "FAILFAST")
  
CREATE TEMPORARY TABLE temp_mtcars_parquet
  USING parquet
  OPTIONS (path "/home/triamus/data/other/mtcars.parquet", header "true", mode "FAILFAST")
```

![](~/screenshots/chrome_zeppelin_sql_create_table.png)

Now we can query it from memory.

```{sql}
%spark.sql

select 
     csv.cyl, 
     csv.gear,
     parquet.hp,
     parquet.drat
   from 
     temp_mtcars_csv as csv
   left join
     temp_mtcars_parquet as parquet
   on
     csv.mpg = parquet.mpg
   where
     csv.gear = 4
   limit 5
```

# Drill to create Parquet files

Assuming we have multiple csv of the same format that we like to append to one big csv and then convert to `parquet` format.

```{bash}
# pipe all headers into one file and inspect manually
for file in $( ls ); do
head -1 $file >> headers
done
# the same in one line
for file in $( ls ); do head -1 $file >> headers; done
rm headers
# append via a shell script
# https://stackoverflow.com/questions/24641948/merging-csv-files-appending-instead-of-merging/24643455
#!/bin/bash
OutFileName="On_Time_Performance_2014-2015.csv"   # Fix the output name
i=0                                       # Reset a counter
for filename in ./*.csv; do 
 if [ "$filename"  != "$OutFileName" ] ;      # Avoid recursion 
 then 
   if [[ $i -eq 0 ]] ; then 
      head -1  $filename >   $OutFileName # Copy header if it is the first file
   fi
   tail -n +2  $filename >>  $OutFileName # Append from the 2nd line each file
   i=$(( $i + 1 ))                        # Increase the counter
 fi
done
```

We have one big csv with ~ 5.2 GB in size. From this, we create a parquet file.

```{sql}
%drill

create table
  dfs.data.`/blue_yonder_airlines/On_Time_Performance_2014-2015/On_Time_Performance_2014-2015.parquet` 
as select
  * 
from 
  dfs.data.`blue_yonder_airlines/On_Time_Performance_2014-2015/On_Time_On_Time_Performance_all.csv`
```

```{sql}
%drill

select
  count(*)
from
  dfs.data.`/blue_yonder_airlines/On_Time_Performance_2014-2015/On_Time_Performance_2014-2015.parquet`
```


```{sql}
drill_query(
  dc, 
  "create table
  dfs.tmp.`/blue_yonder_airlines/On_Time_Performance_2014-2015.parquet` 
as select
  * 
from 
  dfs.tmp.`blue_yonder_airlines/On_Time_Performance_2014-2015/On_Time_On_Time_Performance_all.csv`")
```

* [The Server Side: Apache Drill case study: A tutorial on processing CSV files](https://www.theserverside.com/tutorial/Apache-Drill-case-study-A-tutorial-on-processing-CSV-files)

https://www.slideshare.net/HadoopSummit/spark-sql-versus-apache-drill-different-tools-with-different-rules
https://www.dezyre.com/article/spark-sql-vs-apache-drill-war-of-the-sql-on-hadoop-tools/234
https://www.smartdatacollective.com/apache-drill-vs-apache-spark-what-s-right-tool-job/
https://www.quora.com/Which-is-more-efficient-Spark-over-Hadoop-or-Apache-Drill
https://www.youtube.com/watch?v=PkRmLEi_wu4
https://www.youtube.com/watch?v=Ud_adu9xNLI
https://dzone.com/articles/apache-drill-vs-amazon-athena-a-comparison-on-data


* [rittmanmead: SQL-on-Hadoop: Impala vs Drill](https://www.rittmanmead.com/blog/2017/04/sql-on-hadoop-impala-vs-drill/)

https://www.bigdata-toronto.com/2016/assets/getting_started_with_apache_spark.pdf

## spark sql

https://hortonworks.com/tutorial/learning-spark-sql-with-zeppelin/

## others

* [Rolling Your Own Jupyter and RStudio Data Analysis Environment Around Apache Drill Using docker-compose](https://blog.ouseful.info/2017/06/16/rolling-your-own-jupyter-and-rstudio-data-analysis-environment-around-apache-drill-using-docker-compose/)
* [Tinkering With Apache Drill – JOINed Queries Across JSON and CSV files](https://blog.ouseful.info/2017/06/14/tinkering-with-apache-drill-joined-queries-across-json-and-csv-files/)
* [Querying Large CSV Files With Apache Drill](https://blog.ouseful.info/2017/06/03/querying-large-csv-files-with-apache-drill/)
* [How to combine relational and NoSQL datasets with Apache Drill](http://www.bigendiandata.com/2017-05-01-Apache_Drill/)
* [Exploring Azure Data with Apache Drill, Now Pre-Installed on the Microsoft Data Science Virtual Machine](https://blogs.technet.microsoft.com/machinelearning/2016/12/14/exploring-azure-data-with-apache-drill-now-part-of-the-microsoft-data-science-virtual-machine/)
* [Deploying Drill on MapR in the Azure Cloud (Part 1)](https://mapr.com/blog/deploying-drill-mapr-azure-cloud-part-1/)
* [Connecting a Drill-enabled MapR Cluster to Azure Resources (Part 2)](https://mapr.com/blog/connecting-drill-enabled-mapr-cluster-azure-resources-part-2/)
* [Connecting to Apache Drill with Power BI (Part 3)](https://mapr.com/blog/connecting-apache-drill-power-bi-part-3/)
* [Third Eye: Apache Drill](https://thirdeyedata.io/apache-drill/)
* [Using Apache Drill to Query Parliament Written Questions Data](https://github.com/psychemedia/parlihacks/blob/master/notebooks/Apache%20Drill%20-%20JSON%20Written%20Questions.ipynb)
* [Accessing Apache Drill in Python](https://medium.com/@ApacheDrill/accessing-apache-drill-in-python-15482190d684)
* [Different Ways Of Querying SQL To Hadoop](https://medium.com/@arogyalokeshvutukuru/different-ways-of-querying-sql-to-hadoop-fa0cb809afa0)
* [10 things I wish someone had told me before I started using Apache SparkR](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/1792412399382575/3601578643761083/latest.html)

# Drill on Azure

* [Deploying Apache Drill on Azure](https://blogs.msdn.microsoft.com/data_otaku/2016/05/27/deploying-apache-drill-on-azure/)